
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font than Computer Modern for most use cases
    \usepackage{palatino}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Operations\_on\_word\_vectors\_v2a}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Operations on word vectors}\label{operations-on-word-vectors}

Welcome to your first assignment of this week!

Because word embeddings are very computationally expensive to train,
most ML practitioners will load a pre-trained set of embeddings.

\textbf{After this assignment you will be able to:}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Load pre-trained word vectors, and measure similarity using cosine
  similarity
\item
  Use word embeddings to solve word analogy problems such as Man is to
  Woman as King is to \_\_\_\_\_\_.
\item
  Modify word embeddings to reduce their gender bias
\end{itemize}

    \subsection{Updates}\label{updates}

\paragraph{If you were working on the notebook before this
update\ldots{}}\label{if-you-were-working-on-the-notebook-before-this-update}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  The current notebook is version ``2a''.
\item
  You can find your original work saved in the notebook with the
  previous version name (``v2'')
\item
  To view the file directory, go to the menu
  ``File-\textgreater{}Open'', and this will open a new tab that shows
  the file directory.
\end{itemize}

\paragraph{List of updates}\label{list-of-updates}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  cosine\_similarity

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Additional hints.
  \end{itemize}
\item
  complete\_analogy

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Replaces the list of input words with a set, and sets it outside the
    for loop (to follow best practices in coding).
  \end{itemize}
\item
  Spelling, grammar and wording corrections.
\end{itemize}

    Let's get started! Run the following cell to load the packages you will
need.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{from} \PY{n+nn}{w2v\PYZus{}utils} \PY{k}{import} \PY{o}{*}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Using TensorFlow backend.

    \end{Verbatim}

    \paragraph{Load the word vectors}\label{load-the-word-vectors}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  For this assignment, we will use 50-dimensional GloVe vectors to
  represent words.
\item
  Run the following cell to load the \texttt{word\_to\_vec\_map}.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{words}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map} \PY{o}{=} \PY{n}{read\PYZus{}glove\PYZus{}vecs}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../../readonly/glove.6B.50d.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    You've loaded: - \texttt{words}: set of words in the vocabulary. -
\texttt{word\_to\_vec\_map}: dictionary mapping words to their GloVe
vector representation.

\paragraph{Embedding vectors versus one-hot
vectors}\label{embedding-vectors-versus-one-hot-vectors}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Recall from the lesson videos that one-hot vectors do not do a good
  job of capturing the level of similarity between words (every one-hot
  vector has the same Euclidean distance from any other one-hot vector).
\item
  Embedding vectors such as GloVe vectors provide much more useful
  information about the meaning of individual words.
\item
  Lets now see how you can use GloVe vectors to measure the similarity
  between two words.
\end{itemize}

    \section{1 - Cosine similarity}\label{cosine-similarity}

To measure the similarity between two words, we need a way to measure
the degree of similarity between two embedding vectors for the two
words. Given two vectors $u$ and $v$, cosine similarity is defined as
follows:

\[\text{CosineSimilarity(u, v)} = \frac {u \cdot v} {||u||_2 ||v||_2} = cos(\theta) \tag{1}\]

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  $u \cdot v$ is the dot product (or inner product) of two vectors
\item
  $||u||_2$ is the norm (or length) of the vector $u$
\item
  $\theta$ is the angle between $u$ and $v$.
\item
  The cosine similarity depends on the angle between $u$ and $v$.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    If $u$ and $v$ are very similar, their cosine similarity will be
    close to 1.
  \item
    If they are dissimilar, the cosine similarity will take a smaller
    value.
  \end{itemize}
\end{itemize}

\textbf{Figure 1}: The cosine of the angle between two vectors is a
measure their similarity

\textbf{Exercise}: Implement the function \texttt{cosine\_similarity()}
to evaluate the similarity between word vectors.

\textbf{Reminder}: The norm of $u$ is defined as \$
\textbar{}\textbar{}u\textbar{}\textbar{}\_2 =
\sqrt{\sum_{i=1}^{n} u_i^2}\$

\paragraph{Additional Hints}\label{additional-hints}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  You may find \texttt{np.dot}, \texttt{np.sum}, or \texttt{np.sqrt}
  useful depending upon the implementation that you choose.
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} GRADED FUNCTION: cosine\PYZus{}similarity}
        
        \PY{k}{def} \PY{n+nf}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{u}\PY{p}{,} \PY{n}{v}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Cosine similarity reflects the degree of similarity between u and v}
        \PY{l+s+sd}{        }
        \PY{l+s+sd}{    Arguments:}
        \PY{l+s+sd}{        u \PYZhy{}\PYZhy{} a word vector of shape (n,)          }
        \PY{l+s+sd}{        v \PYZhy{}\PYZhy{} a word vector of shape (n,)}
        
        \PY{l+s+sd}{    Returns:}
        \PY{l+s+sd}{        cosine\PYZus{}similarity \PYZhy{}\PYZhy{} the cosine similarity between u and v defined by the formula above.}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            
            \PY{n}{distance} \PY{o}{=} \PY{l+m+mf}{0.0}
            
            \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
            \PY{c+c1}{\PYZsh{} Compute the dot product between u and v (≈1 line)}
            \PY{n}{dot} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{u}\PY{p}{,} \PY{n}{v}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} Compute the L2 norm of u (≈1 line)}
            \PY{n}{norm\PYZus{}u} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{u}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} Compute the L2 norm of v (≈1 line)}
            \PY{n}{norm\PYZus{}v} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{v}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} Compute the cosine similarity defined by formula (1) (≈1 line)}
            \PY{n}{cosine\PYZus{}similarity} \PY{o}{=} \PY{n}{dot} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{norm\PYZus{}u}\PY{p}{,} \PY{n}{norm\PYZus{}v}\PY{p}{)}
            \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
            
            \PY{k}{return} \PY{n}{cosine\PYZus{}similarity}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{father} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{father}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
        \PY{n}{mother} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mother}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
        \PY{n}{ball} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ball}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
        \PY{n}{crocodile} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{crocodile}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
        \PY{n}{france} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{france}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
        \PY{n}{italy} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{italy}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
        \PY{n}{paris} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{paris}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
        \PY{n}{rome} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rome}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(father, mother) = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{father}\PY{p}{,} \PY{n}{mother}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(ball, crocodile) = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{ball}\PY{p}{,} \PY{n}{crocodile}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(france \PYZhy{} paris, rome \PYZhy{} italy) = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{france} \PY{o}{\PYZhy{}} \PY{n}{paris}\PY{p}{,} \PY{n}{rome} \PY{o}{\PYZhy{}} \PY{n}{italy}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
cosine\_similarity(father, mother) =  0.890903844289
cosine\_similarity(ball, crocodile) =  0.274392462614
cosine\_similarity(france - paris, rome - italy) =  -0.675147930817

    \end{Verbatim}

    \textbf{Expected Output}:

\textbf{cosine\_similarity(father, mother)} =

0.890903844289

\textbf{cosine\_similarity(ball, crocodile)} =

0.274392462614

\textbf{cosine\_similarity(france - paris, rome - italy)} =

-0.675147930817

    \paragraph{Try different words!}\label{try-different-words}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  After you get the correct expected output, please feel free to modify
  the inputs and measure the cosine similarity between other pairs of
  words!
\item
  Playing around with the cosine similarity of other inputs will give
  you a better sense of how word vectors behave.
\end{itemize}

    \subsection{2 - Word analogy task}\label{word-analogy-task}

\begin{itemize}
\item
  In the word analogy task, we complete the sentence:\\``\emph{a} is to
  \emph{b} as \emph{c} is to \textbf{\_\_\_\_}''.
\item
  An example is:\\ `\emph{man} is to \emph{woman} as \emph{king} is to
  \emph{queen}' .
\item
  We are trying to find a word \emph{d}, such that the associated word
  vectors $e_a, e_b, e_c, e_d$ are related in the following
  manner:\\$e_b - e_a \approx e_d - e_c$
\item
  We will measure the similarity between $e_b - e_a$ and $e_d - e_c$
  using cosine similarity.
\end{itemize}

\textbf{Exercise}: Complete the code below to be able to perform word
analogies!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} GRADED FUNCTION: complete\PYZus{}analogy}
        
        \PY{k}{def} \PY{n+nf}{complete\PYZus{}analogy}\PY{p}{(}\PY{n}{word\PYZus{}a}\PY{p}{,} \PY{n}{word\PYZus{}b}\PY{p}{,} \PY{n}{word\PYZus{}c}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Performs the word analogy task as explained above: a is to b as c is to \PYZus{}\PYZus{}\PYZus{}\PYZus{}. }
        \PY{l+s+sd}{    }
        \PY{l+s+sd}{    Arguments:}
        \PY{l+s+sd}{    word\PYZus{}a \PYZhy{}\PYZhy{} a word, string}
        \PY{l+s+sd}{    word\PYZus{}b \PYZhy{}\PYZhy{} a word, string}
        \PY{l+s+sd}{    word\PYZus{}c \PYZhy{}\PYZhy{} a word, string}
        \PY{l+s+sd}{    word\PYZus{}to\PYZus{}vec\PYZus{}map \PYZhy{}\PYZhy{} dictionary that maps words to their corresponding vectors. }
        \PY{l+s+sd}{    }
        \PY{l+s+sd}{    Returns:}
        \PY{l+s+sd}{    best\PYZus{}word \PYZhy{}\PYZhy{}  the word such that v\PYZus{}b \PYZhy{} v\PYZus{}a is close to v\PYZus{}best\PYZus{}word \PYZhy{} v\PYZus{}c, as measured by cosine similarity}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            
            \PY{c+c1}{\PYZsh{} convert words to lowercase}
            \PY{n}{word\PYZus{}a}\PY{p}{,} \PY{n}{word\PYZus{}b}\PY{p}{,} \PY{n}{word\PYZus{}c} \PY{o}{=} \PY{n}{word\PYZus{}a}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{word\PYZus{}b}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{word\PYZus{}c}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
            \PY{c+c1}{\PYZsh{} Get the word embeddings e\PYZus{}a, e\PYZus{}b and e\PYZus{}c (≈1\PYZhy{}3 lines)}
            \PY{n}{e\PYZus{}a}\PY{p}{,} \PY{n}{e\PYZus{}b}\PY{p}{,} \PY{n}{e\PYZus{}c} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{word\PYZus{}a}\PY{p}{]}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{word\PYZus{}b}\PY{p}{]}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{word\PYZus{}c}\PY{p}{]}
            \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
            
            \PY{n}{words} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{o}{.}\PY{n}{keys}\PY{p}{(}\PY{p}{)}
            \PY{n}{max\PYZus{}cosine\PYZus{}sim} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{100}              \PY{c+c1}{\PYZsh{} Initialize max\PYZus{}cosine\PYZus{}sim to a large negative number}
            \PY{n}{best\PYZus{}word} \PY{o}{=} \PY{k+kc}{None}                   \PY{c+c1}{\PYZsh{} Initialize best\PYZus{}word with None, it will help keep track of the word to output}
        
            \PY{c+c1}{\PYZsh{} to avoid best\PYZus{}word being one of the input words, skip the input words}
            \PY{c+c1}{\PYZsh{} place the input words in a set for faster searching than a list}
            \PY{c+c1}{\PYZsh{} We will re\PYZhy{}use this set of input words inside the for\PYZhy{}loop}
            \PY{n}{input\PYZus{}words\PYZus{}set} \PY{o}{=} \PY{n+nb}{set}\PY{p}{(}\PY{p}{[}\PY{n}{word\PYZus{}a}\PY{p}{,} \PY{n}{word\PYZus{}b}\PY{p}{,} \PY{n}{word\PYZus{}c}\PY{p}{]}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} loop over the whole word vector set}
            \PY{k}{for} \PY{n}{w} \PY{o+ow}{in} \PY{n}{words}\PY{p}{:}        
                \PY{c+c1}{\PYZsh{} to avoid best\PYZus{}word being one of the input words, skip the input words}
                \PY{k}{if} \PY{n}{w} \PY{o+ow}{in} \PY{n}{input\PYZus{}words\PYZus{}set}\PY{p}{:}
                    \PY{k}{continue}
                
                \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
                \PY{c+c1}{\PYZsh{} Compute cosine similarity between the vector (e\PYZus{}b \PYZhy{} e\PYZus{}a) and the vector ((w\PYZsq{}s vector representation) \PYZhy{} e\PYZus{}c)  (≈1 line)}
                \PY{n}{cosine\PYZus{}sim} \PY{o}{=} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{p}{(}\PY{n}{e\PYZus{}b} \PY{o}{\PYZhy{}} \PY{n}{e\PYZus{}a}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{w}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{e\PYZus{}c}\PY{p}{)}\PY{p}{)}
                
                \PY{c+c1}{\PYZsh{} If the cosine\PYZus{}sim is more than the max\PYZus{}cosine\PYZus{}sim seen so far,}
                    \PY{c+c1}{\PYZsh{} then: set the new max\PYZus{}cosine\PYZus{}sim to the current cosine\PYZus{}sim and the best\PYZus{}word to the current word (≈3 lines)}
                \PY{k}{if} \PY{n}{cosine\PYZus{}sim} \PY{o}{\PYZgt{}} \PY{n}{max\PYZus{}cosine\PYZus{}sim}\PY{p}{:}
                    \PY{n}{max\PYZus{}cosine\PYZus{}sim} \PY{o}{=} \PY{n}{cosine\PYZus{}sim}
                    \PY{n}{best\PYZus{}word} \PY{o}{=} \PY{n}{w}
                \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
                
            \PY{k}{return} \PY{n}{best\PYZus{}word}
\end{Verbatim}

    Run the cell below to test your code, this may take 1-2 minutes.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{triads\PYZus{}to\PYZus{}try} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{italy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{italian}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{spain}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{india}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{delhi}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{japan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{man}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{woman}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{boy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{small}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{smaller}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{large}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{]}
        \PY{k}{for} \PY{n}{triad} \PY{o+ow}{in} \PY{n}{triads\PYZus{}to\PYZus{}try}\PY{p}{:}
            \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ \PYZhy{}\PYZgt{} }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ :: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ \PYZhy{}\PYZgt{} }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(} \PY{o}{*}\PY{n}{triad}\PY{p}{,} \PY{n}{complete\PYZus{}analogy}\PY{p}{(}\PY{o}{*}\PY{n}{triad}\PY{p}{,}\PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
italy -> italian :: spain -> spanish
india -> delhi :: japan -> tokyo
man -> woman :: boy -> girl
small -> smaller :: large -> larger

    \end{Verbatim}

    \textbf{Expected Output}:

\textbf{italy -\textgreater{} italian} ::

spain -\textgreater{} spanish

\textbf{india -\textgreater{} delhi} ::

japan -\textgreater{} tokyo

\textbf{man -\textgreater{} woman } ::

boy -\textgreater{} girl

\textbf{small -\textgreater{} smaller } ::

large -\textgreater{} larger

    \begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Once you get the correct expected output, please feel free to modify
  the input cells above to test your own analogies.
\item
  Try to find some other analogy pairs that do work, but also find some
  where the algorithm doesn't give the right answer:

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    For example, you can try small-\textgreater{}smaller as
    big-\textgreater{}?.
  \end{itemize}
\end{itemize}

    \subsubsection{Congratulations!}\label{congratulations}

You've come to the end of the graded portion of the assignment. Here are
the main points you should remember:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Cosine similarity is a good way to compare the similarity between
  pairs of word vectors.

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Note that L2 (Euclidean) distance also works.
  \end{itemize}
\item
  For NLP applications, using a pre-trained set of word vectors is often
  a good way to get started.
\item
  Even though you have finished the graded portions, we recommend you
  take a look at the rest of this notebook to learn about debiasing word
  vectors.
\end{itemize}

Congratulations on finishing the graded portions of this notebook!

    \subsection{3 - Debiasing word vectors
(OPTIONAL/UNGRADED)}\label{debiasing-word-vectors-optionalungraded}

    In the following exercise, you will examine gender biases that can be
reflected in a word embedding, and explore algorithms for reducing the
bias. In addition to learning about the topic of debiasing, this
exercise will also help hone your intuition about what word vectors are
doing. This section involves a bit of linear algebra, though you can
probably complete it even without being an expert in linear algebra, and
we encourage you to give it a shot. This portion of the notebook is
optional and is not graded.

Lets first see how the GloVe word embeddings relate to gender. You will
first compute a vector $g = e_{woman}-e_{man}$, where $e_{woman}$
represents the word vector corresponding to the word \emph{woman}, and
$e_{man}$ corresponds to the word vector corresponding to the word
\emph{man}. The resulting vector $g$ roughly encodes the concept of
``gender''. (You might get a more accurate representation if you compute
$g_1 = e_{mother}-e_{father}$, $g_2 = e_{girl}-e_{boy}$, etc. and
average over them. But just using $e_{woman}-e_{man}$ will give good
enough results for now.)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{g} \PY{o}{=} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{woman}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{man}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{g}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[-0.087144    0.2182     -0.40986    -0.03922    -0.1032      0.94165
 -0.06042     0.32988     0.46144    -0.35962     0.31102    -0.86824
  0.96006     0.01073     0.24337     0.08193    -1.02722    -0.21122
  0.695044   -0.00222     0.29106     0.5053     -0.099454    0.40445
  0.30181     0.1355     -0.0606     -0.07131    -0.19245    -0.06115
 -0.3204      0.07165    -0.13337    -0.25068714 -0.14293    -0.224957
 -0.149       0.048882    0.12191    -0.27362    -0.165476   -0.20426
  0.54376    -0.271425   -0.10245    -0.32108     0.2516     -0.33455
 -0.04371     0.01258   ]

    \end{Verbatim}

    Now, you will consider the cosine similarity of different words with
$g$. Consider what a positive value of similarity means vs a negative
cosine similarity.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n+nb}{print} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{List of names and their similarities with constructed vector:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} girls and boys name}
        \PY{n}{name\PYZus{}list} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{john}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{marie}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sophie}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ronaldo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{priya}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rahul}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{danielle}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reza}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{katy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yasmin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        
        \PY{k}{for} \PY{n}{w} \PY{o+ow}{in} \PY{n}{name\PYZus{}list}\PY{p}{:}
            \PY{n+nb}{print} \PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{w}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
List of names and their similarities with constructed vector:
john -0.23163356146
marie 0.315597935396
sophie 0.318687898594
ronaldo -0.312447968503
priya 0.17632041839
rahul -0.169154710392
danielle 0.243932992163
reza -0.079304296722
katy 0.283106865957
yasmin 0.233138577679

    \end{Verbatim}

    As you can see, female first names tend to have a positive cosine
similarity with our constructed vector $g$, while male first names tend
to have a negative cosine similarity. This is not surprising, and the
result seems acceptable.

But let's try with some other words.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Other words and their similarities:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{word\PYZus{}list} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lipstick}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{guns}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{science}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{arts}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{literature}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{warrior}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{doctor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{receptionist}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                     \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{technology}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fashion}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{teacher}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{engineer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pilot}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{computer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{singer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
        \PY{k}{for} \PY{n}{w} \PY{o+ow}{in} \PY{n}{word\PYZus{}list}\PY{p}{:}
            \PY{n+nb}{print} \PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{n}{w}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Other words and their similarities:
lipstick 0.276919162564
guns -0.18884855679
science -0.0608290654093
arts 0.00818931238588
literature 0.0647250443346
warrior -0.209201646411
doctor 0.118952894109
tree -0.0708939917548
receptionist 0.330779417506
technology -0.131937324476
fashion 0.0356389462577
teacher 0.179209234318
engineer -0.0803928049452
pilot 0.00107644989919
computer -0.103303588739
singer 0.185005181365

    \end{Verbatim}

    Do you notice anything surprising? It is astonishing how these results
reflect certain unhealthy gender stereotypes. For example, ``computer''
is closer to ``man'' while ``literature'' is closer to ``woman''. Ouch!

We'll see below how to reduce the bias of these vectors, using an
algorithm due to \href{https://arxiv.org/abs/1607.06520}{Boliukbasi et
al., 2016}. Note that some word pairs such as ``actor''/``actress'' or
``grandmother''/``grandfather'' should remain gender specific, while
other words such as ``receptionist'' or ``technology'' should be
neutralized, i.e.~not be gender-related. You will have to treat these
two types of words differently when debiasing.

\subsubsection{3.1 - Neutralize bias for non-gender specific
words}\label{neutralize-bias-for-non-gender-specific-words}

The figure below should help you visualize what neutralizing does. If
you're using a 50-dimensional word embedding, the 50 dimensional space
can be split into two parts: The bias-direction $g$, and the remaining
49 dimensions, which we'll call $g_{\perp}$. In linear algebra, we say
that the 49 dimensional $g_{\perp}$ is perpendicular (or ``orthogonal'')
to $g$, meaning it is at 90 degrees to $g$. The neutralization step
takes a vector such as $e_{receptionist}$ and zeros out the component in
the direction of $g$, giving us $e_{receptionist}^{debiased}$.

Even though $g_{\perp}$ is 49 dimensional, given the limitations of what
we can draw on a 2D screen, we illustrate it using a 1 dimensional axis
below.

\textbf{Figure 2}: The word vector for ``receptionist'' represented
before and after applying the neutralize operation.

\textbf{Exercise}: Implement \texttt{neutralize()} to remove the bias of
words such as ``receptionist'' or ``scientist''. Given an input
embedding $e$, you can use the following formulas to compute
$e^{debiased}$:

\[e^{bias\_component} = \frac{e \cdot g}{||g||_2^2} * g\tag{2}\]
\[e^{debiased} = e - e^{bias\_component}\tag{3}\]

If you are an expert in linear algebra, you may recognize
$e^{bias\_component}$ as the projection of $e$ onto the direction $g$.
If you're not an expert in linear algebra, don't worry about this.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k}{def} \PY{n+nf}{neutralize}\PY{p}{(}\PY{n}{word}\PY{p}{,} \PY{n}{g}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Removes the bias of \PYZdq{}word\PYZdq{} by projecting it on the space orthogonal to the bias axis. }
         \PY{l+s+sd}{    This function ensures that gender neutral words are zero in the gender subspace.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Arguments:}
         \PY{l+s+sd}{        word \PYZhy{}\PYZhy{} string indicating the word to debias}
         \PY{l+s+sd}{        g \PYZhy{}\PYZhy{} numpy\PYZhy{}array of shape (50,), corresponding to the bias axis (such as gender)}
         \PY{l+s+sd}{        word\PYZus{}to\PYZus{}vec\PYZus{}map \PYZhy{}\PYZhy{} dictionary mapping words to their corresponding vectors.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Returns:}
         \PY{l+s+sd}{        e\PYZus{}debiased \PYZhy{}\PYZhy{} neutralized word vector representation of the input \PYZdq{}word\PYZdq{}}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{c+c1}{\PYZsh{} Select word vector representation of \PYZdq{}word\PYZdq{}. Use word\PYZus{}to\PYZus{}vec\PYZus{}map. (≈ 1 line)}
             \PY{n}{e} \PY{o}{=} \PY{k+kc}{None}
             
             \PY{c+c1}{\PYZsh{} Compute e\PYZus{}biascomponent using the formula given above. (≈ 1 line)}
             \PY{n}{e\PYZus{}biascomponent} \PY{o}{=} \PY{k+kc}{None}
          
             \PY{c+c1}{\PYZsh{} Neutralize e by subtracting e\PYZus{}biascomponent from it }
             \PY{c+c1}{\PYZsh{} e\PYZus{}debiased should be equal to its orthogonal projection. (≈ 1 line)}
             \PY{n}{e\PYZus{}debiased} \PY{o}{=} \PY{k+kc}{None}
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{k}{return} \PY{n}{e\PYZus{}debiased}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{e} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{receptionist}\PY{l+s+s2}{\PYZdq{}}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine similarity between }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n}{e} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ and g, before neutralizing: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{receptionist}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{e\PYZus{}debiased} \PY{o}{=} \PY{n}{neutralize}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{receptionist}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{g}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine similarity between }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{n}{e} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ and g, after neutralizing: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{e\PYZus{}debiased}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
cosine similarity between receptionist and g, before neutralizing:  0.330779417506

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        TypeError                                 Traceback (most recent call last)

        <ipython-input-11-d07861bf2119> in <module>()
          3 
          4 e\_debiased = neutralize("receptionist", g, word\_to\_vec\_map)
    ----> 5 print("cosine similarity between " + e + " and g, after neutralizing: ", cosine\_similarity(e\_debiased, g))
    

        <ipython-input-3-c2b088eef862> in cosine\_similarity(u, v)
         17     \#\#\# START CODE HERE \#\#\#
         18     \# Compute the dot product between u and v (≈1 line)
    ---> 19     dot = np.dot(u, v)
         20     \# Compute the L2 norm of u (≈1 line)
         21     norm\_u = np.sqrt(np.sum(u**2))


        TypeError: unsupported operand type(s) for *: 'NoneType' and 'float'

    \end{Verbatim}

    \textbf{Expected Output}: The second result is essentially 0, up to
numerical rounding (on the order of $10^{-17}$).

\textbf{cosine similarity between receptionist and g, before
neutralizing:} :

0.330779417506

\textbf{cosine similarity between receptionist and g, after
neutralizing:} :

-3.26732746085e-17

    \subsubsection{3.2 - Equalization algorithm for gender-specific
words}\label{equalization-algorithm-for-gender-specific-words}

Next, lets see how debiasing can also be applied to word pairs such as
``actress'' and ``actor.'' Equalization is applied to pairs of words
that you might want to have differ only through the gender property. As
a concrete example, suppose that ``actress'' is closer to ``babysit''
than ``actor.'' By applying neutralizing to ``babysit'' we can reduce
the gender-stereotype associated with babysitting. But this still does
not guarantee that ``actor'' and ``actress'' are equidistant from
``babysit.'' The equalization algorithm takes care of this.

The key idea behind equalization is to make sure that a particular pair
of words are equi-distant from the 49-dimensional $g_\perp$. The
equalization step also ensures that the two equalized steps are now the
same distance from $e_{receptionist}^{debiased}$, or from any other work
that has been neutralized. In pictures, this is how equalization works:

The derivation of the linear algebra to do this is a bit more complex.
(See Bolukbasi et al., 2016 for details.) But the key equations are:

\[ \mu = \frac{e_{w1} + e_{w2}}{2}\tag{4}\]

\[ \mu_{B} = \frac {\mu \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}
\tag{5}\]

\[\mu_{\perp} = \mu - \mu_{B} \tag{6}\]

\[ e_{w1B} = \frac {e_{w1} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}
\tag{7}\]
\[ e_{w2B} = \frac {e_{w2} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}
\tag{8}\]

\[e_{w1B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w1B}} - \mu_B} {||(e_{w1} - \mu_{\perp}) - \mu_B||} \tag{9}\]

\[e_{w2B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w2B}} - \mu_B} {||(e_{w2} - \mu_{\perp}) - \mu_B||} \tag{10}\]

\[e_1 = e_{w1B}^{corrected} + \mu_{\perp} \tag{11}\]
\[e_2 = e_{w2B}^{corrected} + \mu_{\perp} \tag{12}\]

\textbf{Exercise}: Implement the function below. Use the equations above
to get the final equalized version of the pair of words. Good luck!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k}{def} \PY{n+nf}{equalize}\PY{p}{(}\PY{n}{pair}\PY{p}{,} \PY{n}{bias\PYZus{}axis}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
         \PY{l+s+sd}{    Debias gender specific words by following the equalize method described in the figure above.}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Arguments:}
         \PY{l+s+sd}{    pair \PYZhy{}\PYZhy{} pair of strings of gender specific words to debias, e.g. (\PYZdq{}actress\PYZdq{}, \PYZdq{}actor\PYZdq{}) }
         \PY{l+s+sd}{    bias\PYZus{}axis \PYZhy{}\PYZhy{} numpy\PYZhy{}array of shape (50,), vector corresponding to the bias axis, e.g. gender}
         \PY{l+s+sd}{    word\PYZus{}to\PYZus{}vec\PYZus{}map \PYZhy{}\PYZhy{} dictionary mapping words to their corresponding vectors}
         \PY{l+s+sd}{    }
         \PY{l+s+sd}{    Returns}
         \PY{l+s+sd}{    e\PYZus{}1 \PYZhy{}\PYZhy{} word vector corresponding to the first word}
         \PY{l+s+sd}{    e\PYZus{}2 \PYZhy{}\PYZhy{} word vector corresponding to the second word}
         \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
             
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} START CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             \PY{c+c1}{\PYZsh{} Step 1: Select word vector representation of \PYZdq{}word\PYZdq{}. Use word\PYZus{}to\PYZus{}vec\PYZus{}map. (≈ 2 lines)}
             \PY{n}{w1}\PY{p}{,} \PY{n}{w2} \PY{o}{=} \PY{k+kc}{None}
             \PY{n}{e\PYZus{}w1}\PY{p}{,} \PY{n}{e\PYZus{}w2} \PY{o}{=} \PY{k+kc}{None}
             
             \PY{c+c1}{\PYZsh{} Step 2: Compute the mean of e\PYZus{}w1 and e\PYZus{}w2 (≈ 1 line)}
             \PY{n}{mu} \PY{o}{=} \PY{k+kc}{None}
         
             \PY{c+c1}{\PYZsh{} Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)}
             \PY{n}{mu\PYZus{}B} \PY{o}{=} \PY{k+kc}{None}
             \PY{n}{mu\PYZus{}orth} \PY{o}{=} \PY{k+kc}{None}
         
             \PY{c+c1}{\PYZsh{} Step 4: Use equations (7) and (8) to compute e\PYZus{}w1B and e\PYZus{}w2B (≈2 lines)}
             \PY{n}{e\PYZus{}w1B} \PY{o}{=} \PY{k+kc}{None}
             \PY{n}{e\PYZus{}w2B} \PY{o}{=} \PY{k+kc}{None}
                 
             \PY{c+c1}{\PYZsh{} Step 5: Adjust the Bias part of e\PYZus{}w1B and e\PYZus{}w2B using the formulas (9) and (10) given above (≈2 lines)}
             \PY{n}{corrected\PYZus{}e\PYZus{}w1B} \PY{o}{=} \PY{k+kc}{None}
             \PY{n}{corrected\PYZus{}e\PYZus{}w2B} \PY{o}{=} \PY{k+kc}{None}
         
             \PY{c+c1}{\PYZsh{} Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)}
             \PY{n}{e1} \PY{o}{=} \PY{k+kc}{None}
             \PY{n}{e2} \PY{o}{=} \PY{k+kc}{None}
                                                                         
             \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} END CODE HERE \PYZsh{}\PYZsh{}\PYZsh{}}
             
             \PY{k}{return} \PY{n}{e1}\PY{p}{,} \PY{n}{e2}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine similarities before equalizing:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(word\PYZus{}to\PYZus{}vec\PYZus{}map[}\PY{l+s+se}{\PYZbs{}\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+se}{\PYZbs{}\PYZdq{}}\PY{l+s+s2}{], gender) = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(word\PYZus{}to\PYZus{}vec\PYZus{}map[}\PY{l+s+se}{\PYZbs{}\PYZdq{}}\PY{l+s+s2}{woman}\PY{l+s+se}{\PYZbs{}\PYZdq{}}\PY{l+s+s2}{], gender) = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{woman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n}{e1}\PY{p}{,} \PY{n}{e2} \PY{o}{=} \PY{n}{equalize}\PY{p}{(}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{man}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{woman}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{,} \PY{n}{g}\PY{p}{,} \PY{n}{word\PYZus{}to\PYZus{}vec\PYZus{}map}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine similarities after equalizing:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(e1, gender) = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{e1}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cosine\PYZus{}similarity(e2, gender) = }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{cosine\PYZus{}similarity}\PY{p}{(}\PY{n}{e2}\PY{p}{,} \PY{n}{g}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
cosine similarities before equalizing:
cosine\_similarity(word\_to\_vec\_map["man"], gender) =  -0.117110957653
cosine\_similarity(word\_to\_vec\_map["woman"], gender) =  0.356666188463


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        TypeError                                 Traceback (most recent call last)

        <ipython-input-13-c9e1cd932a24> in <module>()
          3 print("cosine\_similarity(word\_to\_vec\_map[\textbackslash{}"woman\textbackslash{}"], gender) = ", cosine\_similarity(word\_to\_vec\_map["woman"], g))
          4 print()
    ----> 5 e1, e2 = equalize(("man", "woman"), g, word\_to\_vec\_map)
          6 print("cosine similarities after equalizing:")
          7 print("cosine\_similarity(e1, gender) = ", cosine\_similarity(e1, g))


        <ipython-input-12-d216528a872b> in equalize(pair, bias\_axis, word\_to\_vec\_map)
         15     \#\#\# START CODE HERE \#\#\#
         16     \# Step 1: Select word vector representation of "word". Use word\_to\_vec\_map. (≈ 2 lines)
    ---> 17     w1, w2 = None
         18     e\_w1, e\_w2 = None
         19 


        TypeError: 'NoneType' object is not iterable

    \end{Verbatim}

    \textbf{Expected Output}:

cosine similarities before equalizing:

\textbf{cosine\_similarity(word\_to\_vec\_map{[}``man''{]}, gender)} =

-0.117110957653

\textbf{cosine\_similarity(word\_to\_vec\_map{[}``woman''{]}, gender)} =

0.356666188463

cosine similarities after equalizing:

\textbf{cosine\_similarity(u1, gender)} =

-0.700436428931

\textbf{cosine\_similarity(u2, gender)} =

0.700436428931

    Please feel free to play with the input words in the cell above, to
apply equalization to other pairs of words.

These debiasing algorithms are very helpful for reducing bias, but are
not perfect and do not eliminate all traces of bias. For example, one
weakness of this implementation was that the bias direction $g$ was
defined using only the pair of words \emph{woman} and \emph{man}. As
discussed earlier, if $g$ were defined by computing
$g_1 = e_{woman} - e_{man}$; $g_2 = e_{mother} - e_{father}$;
$g_3 = e_{girl} - e_{boy}$; and so on and averaging over them, you would
obtain a better estimate of the ``gender'' dimension in the 50
dimensional word embedding space. Feel free to play with such variants
as well.

    \subsubsection{Congratulations}\label{congratulations}

You have come to the end of this notebook, and have seen a lot of the
ways that word vectors can be used as well as modified.

Congratulations on finishing this notebook!

    \textbf{References}: - The debiasing algorithm is from Bolukbasi et al.,
2016,
\href{https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf}{Man
is to Computer Programmer as Woman is to Homemaker? Debiasing Word
Embeddings} - The GloVe word embeddings were due to Jeffrey Pennington,
Richard Socher, and Christopher D. Manning.
(https://nlp.stanford.edu/projects/glove/)


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
